{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = StandardScaler().fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.99, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca = pca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 54\n"
     ]
    }
   ],
   "source": [
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize features by removing the mean and scaling to unit variance\n",
      "\n",
      "    The standard score of a sample `x` is calculated as:\n",
      "\n",
      "        z = (x - u) / s\n",
      "\n",
      "    where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      "    and `s` is the standard deviation of the training samples or one if\n",
      "    `with_std=False`.\n",
      "\n",
      "    Centering and scaling happen independently on each feature by computing\n",
      "    the relevant statistics on the samples in the training set. Mean and\n",
      "    standard deviation are then stored to be used on later data using\n",
      "    :meth:`transform`.\n",
      "\n",
      "    Standardization of a dataset is a common requirement for many\n",
      "    machine learning estimators: they might behave badly if the\n",
      "    individual features do not more or less look like standard normally\n",
      "    distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      "\n",
      "    For instance many elements used in the objective function of\n",
      "    a learning algorithm (such as the RBF kernel of Support Vector\n",
      "    Machines or the L1 and L2 regularizers of linear models) assume that\n",
      "    all features are centered around 0 and have variance in the same\n",
      "    order. If a feature has a variance that is orders of magnitude larger\n",
      "    that others, it might dominate the objective function and make the\n",
      "    estimator unable to learn from other features correctly as expected.\n",
      "\n",
      "    This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      "    `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      "\n",
      "    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    copy : boolean, optional, default True\n",
      "        If False, try to avoid a copy and do inplace scaling instead.\n",
      "        This is not guaranteed to always work inplace; e.g. if the data is\n",
      "        not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      "        returned.\n",
      "\n",
      "    with_mean : boolean, True by default\n",
      "        If True, center the data before scaling.\n",
      "        This does not work (and will raise an exception) when attempted on\n",
      "        sparse matrices, because centering them entails building a dense\n",
      "        matrix which in common use cases is likely to be too large to fit in\n",
      "        memory.\n",
      "\n",
      "    with_std : boolean, True by default\n",
      "        If True, scale the data to unit variance (or equivalently,\n",
      "        unit standard deviation).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    scale_ : ndarray or None, shape (n_features,)\n",
      "        Per feature relative scaling of the data. This is calculated using\n",
      "        `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *scale_*\n",
      "\n",
      "    mean_ : ndarray or None, shape (n_features,)\n",
      "        The mean value for each feature in the training set.\n",
      "        Equal to ``None`` when ``with_mean=False``.\n",
      "\n",
      "    var_ : ndarray or None, shape (n_features,)\n",
      "        The variance for each feature in the training set. Used to compute\n",
      "        `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      "\n",
      "    n_samples_seen_ : int or array, shape (n_features,)\n",
      "        The number of samples processed by the estimator for each feature.\n",
      "        If there are not missing samples, the ``n_samples_seen`` will be an\n",
      "        integer, otherwise it will be an array.\n",
      "        Will be reset on new calls to fit, but increments across\n",
      "        ``partial_fit`` calls.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.preprocessing import StandardScaler\n",
      "    >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      "    >>> scaler = StandardScaler()\n",
      "    >>> print(scaler.fit(data))\n",
      "    StandardScaler()\n",
      "    >>> print(scaler.mean_)\n",
      "    [0.5 0.5]\n",
      "    >>> print(scaler.transform(data))\n",
      "    [[-1. -1.]\n",
      "     [-1. -1.]\n",
      "     [ 1.  1.]\n",
      "     [ 1.  1.]]\n",
      "    >>> print(scaler.transform([[2, 2]]))\n",
      "    [[3. 3.]]\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    scale: Equivalent function without the estimator API.\n",
      "\n",
      "    :class:`sklearn.decomposition.PCA`\n",
      "        Further removes the linear correlation across features with 'whiten=True'.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "    transform.\n",
      "\n",
      "    We use a biased estimator for the standard deviation, equivalent to\n",
      "    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "    affect model performance.\n",
      "\n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal component analysis (PCA).\n",
      "\n",
      "    Linear dimensionality reduction using Singular Value Decomposition of the\n",
      "    data to project it to a lower dimensional space. The input data is centered\n",
      "    but not scaled for each feature before applying the SVD.\n",
      "\n",
      "    It uses the LAPACK implementation of the full SVD or a randomized truncated\n",
      "    SVD by the method of Halko et al. 2009, depending on the shape of the input\n",
      "    data and the number of components to extract.\n",
      "\n",
      "    It can also use the scipy.sparse.linalg ARPACK implementation of the\n",
      "    truncated SVD.\n",
      "\n",
      "    Notice that this class does not support sparse input. See\n",
      "    :class:`TruncatedSVD` for an alternative with sparse data.\n",
      "\n",
      "    Read more in the :ref:`User Guide <PCA>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_components : int, float, None or str\n",
      "        Number of components to keep.\n",
      "        if n_components is not set all components are kept::\n",
      "\n",
      "            n_components == min(n_samples, n_features)\n",
      "\n",
      "        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n",
      "        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n",
      "        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n",
      "\n",
      "        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n",
      "        number of components such that the amount of variance that needs to be\n",
      "        explained is greater than the percentage specified by n_components.\n",
      "\n",
      "        If ``svd_solver == 'arpack'``, the number of components must be\n",
      "        strictly less than the minimum of n_features and n_samples.\n",
      "\n",
      "        Hence, the None case results in::\n",
      "\n",
      "            n_components == min(n_samples, n_features) - 1\n",
      "\n",
      "    copy : bool, default=True\n",
      "        If False, data passed to fit are overwritten and running\n",
      "        fit(X).transform(X) will not yield the expected results,\n",
      "        use fit_transform(X) instead.\n",
      "\n",
      "    whiten : bool, optional (default False)\n",
      "        When True (False by default) the `components_` vectors are multiplied\n",
      "        by the square root of n_samples and then divided by the singular values\n",
      "        to ensure uncorrelated outputs with unit component-wise variances.\n",
      "\n",
      "        Whitening will remove some information from the transformed signal\n",
      "        (the relative variance scales of the components) but can sometime\n",
      "        improve the predictive accuracy of the downstream estimators by\n",
      "        making their data respect some hard-wired assumptions.\n",
      "\n",
      "    svd_solver : str {'auto', 'full', 'arpack', 'randomized'}\n",
      "        If auto :\n",
      "            The solver is selected by a default policy based on `X.shape` and\n",
      "            `n_components`: if the input data is larger than 500x500 and the\n",
      "            number of components to extract is lower than 80% of the smallest\n",
      "            dimension of the data, then the more efficient 'randomized'\n",
      "            method is enabled. Otherwise the exact full SVD is computed and\n",
      "            optionally truncated afterwards.\n",
      "        If full :\n",
      "            run exact full SVD calling the standard LAPACK solver via\n",
      "            `scipy.linalg.svd` and select the components by postprocessing\n",
      "        If arpack :\n",
      "            run SVD truncated to n_components calling ARPACK solver via\n",
      "            `scipy.sparse.linalg.svds`. It requires strictly\n",
      "            0 < n_components < min(X.shape)\n",
      "        If randomized :\n",
      "            run randomized SVD by the method of Halko et al.\n",
      "\n",
      "        .. versionadded:: 0.18.0\n",
      "\n",
      "    tol : float >= 0, optional (default .0)\n",
      "        Tolerance for singular values computed by svd_solver == 'arpack'.\n",
      "\n",
      "        .. versionadded:: 0.18.0\n",
      "\n",
      "    iterated_power : int >= 0, or 'auto', (default 'auto')\n",
      "        Number of iterations for the power method computed by\n",
      "        svd_solver == 'randomized'.\n",
      "\n",
      "        .. versionadded:: 0.18.0\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.\n",
      "\n",
      "        .. versionadded:: 0.18.0\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    components_ : array, shape (n_components, n_features)\n",
      "        Principal axes in feature space, representing the directions of\n",
      "        maximum variance in the data. The components are sorted by\n",
      "        ``explained_variance_``.\n",
      "\n",
      "    explained_variance_ : array, shape (n_components,)\n",
      "        The amount of variance explained by each of the selected components.\n",
      "\n",
      "        Equal to n_components largest eigenvalues\n",
      "        of the covariance matrix of X.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    explained_variance_ratio_ : array, shape (n_components,)\n",
      "        Percentage of variance explained by each of the selected components.\n",
      "\n",
      "        If ``n_components`` is not set then all components are stored and the\n",
      "        sum of the ratios is equal to 1.0.\n",
      "\n",
      "    singular_values_ : array, shape (n_components,)\n",
      "        The singular values corresponding to each of the selected components.\n",
      "        The singular values are equal to the 2-norms of the ``n_components``\n",
      "        variables in the lower-dimensional space.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    mean_ : array, shape (n_features,)\n",
      "        Per-feature empirical mean, estimated from the training set.\n",
      "\n",
      "        Equal to `X.mean(axis=0)`.\n",
      "\n",
      "    n_components_ : int\n",
      "        The estimated number of components. When n_components is set\n",
      "        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n",
      "        number is estimated from input data. Otherwise it equals the parameter\n",
      "        n_components, or the lesser value of n_features and n_samples\n",
      "        if n_components is None.\n",
      "\n",
      "    n_features_ : int\n",
      "        Number of features in the training data.\n",
      "\n",
      "    n_samples_ : int\n",
      "        Number of samples in the training data.\n",
      "\n",
      "    noise_variance_ : float\n",
      "        The estimated noise covariance following the Probabilistic PCA model\n",
      "        from Tipping and Bishop 1999. See \"Pattern Recognition and\n",
      "        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n",
      "        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n",
      "        compute the estimated data covariance and score samples.\n",
      "\n",
      "        Equal to the average of (min(n_features, n_samples) - n_components)\n",
      "        smallest eigenvalues of the covariance matrix of X.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    KernelPCA : Kernel Principal Component Analysis.\n",
      "    SparsePCA : Sparse Principal Component Analysis.\n",
      "    TruncatedSVD : Dimensionality reduction using truncated SVD.\n",
      "    IncrementalPCA : Incremental Principal Component Analysis.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    For n_components == 'mle', this class uses the method of *Minka, T. P.\n",
      "    \"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604*\n",
      "\n",
      "    Implements the probabilistic PCA model from:\n",
      "    Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n",
      "    component analysis\". Journal of the Royal Statistical Society:\n",
      "    Series B (Statistical Methodology), 61(3), 611-622.\n",
      "    via the score and score_samples methods.\n",
      "    See http://www.miketipping.com/papers/met-mppca.pdf\n",
      "\n",
      "    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n",
      "\n",
      "    For svd_solver == 'randomized', see:\n",
      "    *Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n",
      "    \"Finding structure with randomness: Probabilistic algorithms for\n",
      "    constructing approximate matrix decompositions\".\n",
      "    SIAM review, 53(2), 217-288.* and also\n",
      "    *Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n",
      "    \"A randomized algorithm for the decomposition of matrices\".\n",
      "    Applied and Computational Harmonic Analysis, 30(1), 47-68.*\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.decomposition import PCA\n",
      "    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "    >>> pca = PCA(n_components=2)\n",
      "    >>> pca.fit(X)\n",
      "    PCA(n_components=2)\n",
      "    >>> print(pca.explained_variance_ratio_)\n",
      "    [0.9924... 0.0075...]\n",
      "    >>> print(pca.singular_values_)\n",
      "    [6.30061... 0.54980...]\n",
      "\n",
      "    >>> pca = PCA(n_components=2, svd_solver='full')\n",
      "    >>> pca.fit(X)\n",
      "    PCA(n_components=2, svd_solver='full')\n",
      "    >>> print(pca.explained_variance_ratio_)\n",
      "    [0.9924... 0.00755...]\n",
      "    >>> print(pca.singular_values_)\n",
      "    [6.30061... 0.54980...]\n",
      "\n",
      "    >>> pca = PCA(n_components=1, svd_solver='arpack')\n",
      "    >>> pca.fit(X)\n",
      "    PCA(n_components=1, svd_solver='arpack')\n",
      "    >>> print(pca.explained_variance_ratio_)\n",
      "    [0.99244...]\n",
      "    >>> print(pca.singular_values_)\n",
      "    [6.30061...]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(PCA.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit5c18ea34c0db42e4a61590436e1ee6df"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
